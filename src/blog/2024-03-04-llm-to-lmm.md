---
layout: post.njk
title: "Des LLM aux LMM - les modèles multimodaux"
date: 2024-01-04
tags:
  - post
  - AI
  - LLM

description: "Des LLM aux LMM - les modèles multimodaux"
coverImage: /assets/images/blog/llm-to-lmm/hero.webp
---

*Article originalement publié sur [WEnvision](https://www.wenvision.com/fr/articles/des-llm-aux-lmm-les-modeles-multimodaux/)*

Les LLM sont spécialisés dans la donnée textuelle. Ils rencontrent donc des limites lorsqu'il s'agit d'interpréter des données multimodales, qui comprennent non seulement du texte mais aussi des images, des sons et des vidéos. L'avenir de IA générative réside ainsi dans l'utilisation de modèles multimodaux.

Les documents d'entreprise contiennent souvent des éléments visuels comme des images, des graphiques et des schémas. Ces éléments sont riches en informations contextuelles essentielles à la compréhension du document. Les modèles multimodaux, dits LMM, permettent de combler le fossé entre le traitement du langage naturel et la perception visuelle.

En combinant la puissance des LLM avec la capacité de traiter des données visuelles, ces modèles promettent :

  - Une extraction d'informations plus efficace : les modèles multimodaux peuvent extraire automatiquement des informations à partir d'images et de textes, ce qui rationalise la gestion des documents et rend les informations plus accessibles.
  - Des générations plus riches : en intégrant des données visuelles, les modèles multimodaux peuvent générer des textes plus complets.
  - Des recommandations plus personnalisées : dans le domaine du commerce électronique, ces modèles peuvent fournir des recommandations personnalisées en tenant compte des images, des descriptions de produits et des évaluations des utilisateurs.

Exemples d'applications multimodales améliorées par les LMM : rédaction automatique de résumés de documents, recherche d'information multimodale, assistance à la clientèle à partir de photos et de texte, etc.