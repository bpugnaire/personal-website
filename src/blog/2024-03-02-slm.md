---
layout: post.njk
title: "LLM ou SLM ? Les modèles de langage se mettent au régime"
date: 2024-01-02
tags:
  - post
  - AI
  - LLM

description: "LLM ou SLM."
coverImage: /personal-website/assets/images/blog/slm/hero.png
---

*Article originalement publié sur [WEnvision](https://www.wenvision.com/fr/articles/llm-ou-slm-les-modeles-de-langage-se-mettent-au-regime/)*

Travailler avec des Large Language Models implique des contraintes notamment de coûts et d'infrastructure. Les Small Language Models (SLM) se démarquent de leurs homologues plus imposants en offrant une approche plus efficiente, flexible et personnalisable. Pour créer un SLM, on a d'abord besoin d'un LLM pour servir de référence (ou de fondation) et, par un processus de "distillation", on parvient à entraîner un modèle plus compact.

L'objectif poursuivi lors de la création d'un SLM est d'optimiser un rapport bénéfices/coûts : on cherche à maintenir un niveau de performance comparable au LLM de référence en réduisant progressivement le nombre de paramètres nécessaires. 

Une fois entrainés, ces SLM sont beaucoup plus flexibles car ils nécessitent moins de puissance de calcul à l'usage et occupent moins de place dans la mémoire des appareils qui l'embarquent. Cette approche ouvre de nouvelles possibilités pour décentraliser l'utilisation des modèles de langage en permettant leur déploiement dans des systèmes du quotidien comme nos ordinateurs et téléphones portables. D'autre part, la réduction du nombre de calculs nécessaires pour effectuer des prédictions fait des SLM des bons candidats pour les applications nécessitant du temps réel ou une forte réactivité comme la retranscription vocale.

# Des modèles spécialisés, experts dans un domaine

Il est particulièrement difficile de personnaliser un LLM : cette opération peut être instable, nécessite énormément de données de qualité, très coûteuse également… C'est principalement pour cela que si peu d'entreprise sont capables de proposer de réelles alternatives à ChatGPT. Les SLM permettent de faire sauter cette barrière à l'entrée en diminuant toutes ces complexités (proportionnellement à la réduction du nombre de paramètres). On peut donc plus facilement obtenir des modèles spécialisés qui, en moyenne, sont moins bons qu'un modèle généraliste, mais font preuve d'expertise dans leur domaine (un peu comme nous finalement).

Un bénéfice moins connu de cette spécialisation est la diminution des hallucinations : en réduisant le périmètre de compétence, on facilite le choix d'un corpus d'entraînement très pertinent qui limite aussi l'exposition de données sensibles.